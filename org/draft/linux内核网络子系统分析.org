#+STARTUP: overview
#+STARTUP: hidestars
#+OPTIONS:    H:3 num:nil toc:t \n:nil ::t |:t ^:t -:t f:t *:t tex:t d:(HIDE) tags:not-in-toc
#+HTML_HEAD: <link rel="stylesheet" title="Standard" href="css/worg.css" type="text/css" />



* 重要数据结构

** =struct softnet_data=
   #+BEGIN_SRC c
     /*
      ,* Incoming packets are placed on per-cpu queues
      ,*/
     struct softnet_data {
             struct Qdisc            *output_queue;
             struct Qdisc            **output_queue_tailp;
             struct list_head        poll_list;
             struct sk_buff          *completion_queue;
             struct sk_buff_head     process_queue;

             /* stats */
             unsigned int            processed;
             unsigned int            time_squeeze;
             unsigned int            cpu_collision;
             unsigned int            received_rps;

     #ifdef CONFIG_RPS
             struct softnet_data     *rps_ipi_list;

             /* Elements below can be accessed between CPUs for RPS */
             struct call_single_data csd ____cacheline_aligned_in_smp;
             struct softnet_data     *rps_ipi_next;
             unsigned int            cpu;
             unsigned int            input_queue_head;
             unsigned int            input_queue_tail;
     #endif
             unsigned int            dropped;
             //在incoming frame被Driver处理之前，存储的位置
             struct sk_buff_head     input_pkt_queue;
             struct napi_struct      backlog;

     #ifdef CONFIG_NET_FLOW_LIMIT
             struct sd_flow_limit __rcu *flow_limit;
     #endif
     };

     /*
      ,* Structure for NAPI scheduling similar to tasklet but with weighting
      ,*/
     struct napi_struct {
             /* The poll_list must only be managed by the entity which
              ,* changes the state of the NAPI_STATE_SCHED bit.  This means
              ,* whoever atomically sets that bit can add this napi_struct
              ,* to the per-cpu poll_list, and whoever clears that bit
              ,* can remove from the list right before clearing the bit.
              ,*/
             //This is a bidirectional list of devices with input frames waiting to be processed.
             struct list_head        poll_list;

             unsigned long           state;
             int                     weight;
             unsigned int            gro_count;
             int                     (*poll)(struct napi_struct *, int);
     #ifdef CONFIG_NETPOLL
             spinlock_t              poll_lock;
             int                     poll_owner;
     #endif
             //represents a device that has scheduled net_rx_action for execution on the associated CPU.
             struct net_device       *dev;
             struct sk_buff          *gro_list;
             struct sk_buff          *skb;
             struct list_head        dev_list;
             struct hlist_node       napi_hash_node;
             unsigned int            napi_id;
     };
   #+END_SRC
** =struct sk_buff=

** =struct net_device=

* 接收数据

** 概述
   L2层数据包的处理是通过中断来触发的。当中断发生后，内核会调用Driver相应的
   中断处理函数。中断处理函数会立即响应，并将主要工作放到下半部去执行。
   1. 拷贝数据包到一个 =struct sk_buff= 的数据结构里面。 如果使用DMA，
      则只需要初始化一个指针。
   2. 初始化一些 =sk_buff= 所需要的一些参数，供后续上层网络层使用。用
      于标识上层协议处理函数。
   3. 更新其他的一些设备相关的参数。
   4. 通过调度 =NET_RX_SOFTIRQ= 软为断来告知内核有新的数据帧到达。

** 开启和禁用设备
   =net_device->state= 这个值可以显示网络设备当前的状态， 当为 =_
   _LINK_STATE_START= 代表设备已经使能。 =_LINK_STATE_XOFF= 用于显示地
   开启或关闭数据发送。 接收则没有相关的状态标记来开启或关闭。
   =netif_running= 用于检查当前网络设备的运行状态。 

** 队列
   有两个队列，发送队列和接收队列。 每个队列都有一个指向相关设备的指针
   以及存储数据包的 =sk_buff= 数据结构。

** 通知一个数据包的到来
   
*** 通过调用 =netif_rx= 来通知内核
    一般的逻辑如下：
    #+BEGIN_SRC c
      skb = dev_alloc_skb(pkt_len + 5);
      ... ... ...
      if (skb != NULL) {
      skb->dev = dev;
      skb_reserve(skb, 2);
      /* Align IP on 16 byte boundaries */
      ... ... ...
      /* copy the DATA into the sk_buff structure */
      ... ... ...
      skb->protocol = eth_type_trans(skb, dev);
      netif_rx(skb);
      dev->last_rx = jiffies;
      ... ... ...
      }    
    #+END_SRC
*** 利用NAPI新机制
    NAPI与旧机制的不同点主要有两个地方：
    1. 驱动必须提供poll方法。
    2. 调用调用帧的接口
       #+BEGIN_SRC c
         //直接调用此接口通知内核收到数据帧
         netif_rx_schedule(...);

         //或通过分为如下两个调用
         netif_rx_schedule_prep(...)
         __netif_rx_schedule(...)
       #+END_SRC
    当收到类型的Driver都会将接收帧的设备加入到 =poll_list= 队列中，并
    调度 =NET_RX_SOFTIRQ= 软中断执行。 最终会被 =net_rx_action=。 两者
    之间的差异如下图所示：
    [[./images/2016/2016032301.png]]
** =netif_rx= 函数流程图
   =netif_rx_ni= 是非中断环境下运行的版本。
   [[./images/2016/2016032302.png]]
** 下半部处理
   处理 =NET_RX_SOFTIRQ= 软中断消息函数 =net_rx_action= 。
   对于NAPI驱动来说，它会调用driver注册的poll函数。
   [[./images/2016/2016032303.png]] 
   在 =net_dev_init= 初始化阶段， 会注册poll函数为 =process_backlog= 。
** =netif_receive_skb= 
   此函数是处理帧的具体函数，它的执行逻辑大概如下图所示：
   [[./images/2016/2016032401.png]]

   图中提到的Diverter可以修改数据包的目的地址。
   执行到这个函数后，接收到的数据包将会根据需要传递到L3层去处理。 L3层
   会注册相关的协议处理函数。

* 发送数据
